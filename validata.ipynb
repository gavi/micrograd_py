{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0.dev20230111'"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([600.], dtype=torch.float64) tensor([120.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([10], dtype=float,requires_grad=True)\n",
    "y = torch.tensor([20], dtype=float,requires_grad=True)\n",
    "\n",
    "z = 2 * x ** 3 + 3 * y ** 2\n",
    "z.backward()\n",
    "\n",
    "print(x.grad,y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0557], dtype=torch.float64)\n",
      "tensor([0.1114], dtype=torch.float64)\n",
      "tensor([0.1670], dtype=torch.float64)\n",
      "tensor([-0.5461], dtype=torch.float64, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "class Neuron:\n",
    "    def __init__(self, numInputs = 0):\n",
    "#        random.seed(1)\n",
    "        self.numInputs = numInputs\n",
    "        self.weights =[torch.tensor([random.uniform(-1,1)], dtype=float,requires_grad=True) for _ in range(numInputs)]\n",
    "        self.bias = torch.tensor([random.uniform(-1,1)],dtype= float, requires_grad= True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sum = torch.tensor([0],dtype=float)\n",
    "        for wi,xi in zip(self.weights, x):\n",
    "            sum+=wi*xi\n",
    "        return (sum+self.bias).tanh()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "n = Neuron(numInputs=3)\n",
    "#output = n([torch.tensor([1],dtype= float, requires_grad= True),torch.tensor([2],dtype= float, requires_grad= True),torch.tensor([3],dtype= float, requires_grad= True)])\n",
    "output = n([1.0,2.0,3.0])\n",
    "output.backward()\n",
    "\n",
    "for w in n.weights:\n",
    "    print(w.grad)\n",
    "print(n.bias)\n",
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,numInputs=0,numOutputs=0):\n",
    "        self.neurons = [Neuron(numInputs) for _ in range(numOutputs)]\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return [n(x) for n in self.neurons]\n",
    "    \n",
    "    def parameters(self):\n",
    "        p = []\n",
    "        for neuron in self.neurons:\n",
    "            p.extend(neuron.parameters())\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Layer(3,4)\n",
    "x([1,2,3])\n",
    "len(x.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self,numInputs=0, numOutputs=[]):\n",
    "        sz = [numInputs] + numOutputs\n",
    "        self.layers = []\n",
    "        for i in range(len(numOutputs)):\n",
    "            self.layers.append(Layer(sz[i],sz[i+1]))\n",
    "    def __call__(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x[0] if len(x)==1 else p   \n",
    "    def parameters(self):\n",
    "        p = []\n",
    "        for layer in self.layers:\n",
    "            p.extend(layer.parameters())\n",
    "        return p\n",
    "mlp = MLP(3,[4,4,4,1])\n",
    "out = mlp([1,2,3])\n",
    "len(mlp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[2.0,3.0,-1.0],\n",
    "      [3.0,-1.0,0.5],\n",
    "      [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]];\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.5965], dtype=torch.float64)\n",
      "tensor([4.4885], dtype=torch.float64)\n",
      "tensor([3.7026], dtype=torch.float64)\n",
      "tensor([3.9012], dtype=torch.float64)\n",
      "tensor([5.1843], dtype=torch.float64)\n",
      "tensor([3.5682], dtype=torch.float64)\n",
      "tensor([4.5573], dtype=torch.float64)\n",
      "tensor([1.4964], dtype=torch.float64)\n",
      "tensor([0.3814], dtype=torch.float64)\n",
      "tensor([0.1681], dtype=torch.float64)\n",
      "tensor([0.1343], dtype=torch.float64)\n",
      "tensor([0.1117], dtype=torch.float64)\n",
      "tensor([0.0954], dtype=torch.float64)\n",
      "tensor([0.0832], dtype=torch.float64)\n",
      "tensor([0.0737], dtype=torch.float64)\n",
      "tensor([0.0661], dtype=torch.float64)\n",
      "tensor([0.0599], dtype=torch.float64)\n",
      "tensor([0.0547], dtype=torch.float64)\n",
      "tensor([0.0503], dtype=torch.float64)\n",
      "tensor([0.0466], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "yPred = []\n",
    "for epoch in range(20):\n",
    "    yPred = [mlp(x) for x in xs]\n",
    "    loss = sum([(yP-y)**2 for yP,y in zip(yPred,ys)])\n",
    "\n",
    "    #zero grad\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    #backward\n",
    "    loss.backward()\n",
    "\n",
    "    #update params\n",
    "    for p in mlp.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "    print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.8764], dtype=torch.float64, grad_fn=<TanhBackward0>),\n",
       " tensor([-0.9069], dtype=torch.float64, grad_fn=<TanhBackward0>),\n",
       " tensor([-0.8935], dtype=torch.float64, grad_fn=<TanhBackward0>),\n",
       " tensor([0.8938], dtype=torch.float64, grad_fn=<TanhBackward0>)]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.6883], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.5156], dtype=torch.float64, requires_grad=True),\n",
       " tensor([1.1972], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.9718], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.0549], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.0452], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.0214], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.1884], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.8432], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.1579], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.2189], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.8866], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.0846], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.9130], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.6274], dtype=torch.float64, requires_grad=True),\n",
       " tensor([1.0763], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.0731], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.5278], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.4421], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-1.1631], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.4478], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.6930], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.7591], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.8098], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.6110], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.6651], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.3327], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.4052], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.1804], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-1.2802], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.6659], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.9064], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.0843], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-1.0448], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.4558], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.8216], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.9211], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.6450], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.5614], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.9117], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.3305], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.2303], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.8656], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.3855], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.6566], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.1628], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.8218], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.4498], dtype=torch.float64, requires_grad=True),\n",
       " tensor([1.3300], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.8170], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.7815], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.4539], dtype=torch.float64, requires_grad=True),\n",
       " tensor([1.0208], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.3968], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.2691], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.8816], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.0552], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.0298], dtype=torch.float64, requires_grad=True),\n",
       " tensor([1.5358], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.2259], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-0.1943], dtype=torch.float64, requires_grad=True)]"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
